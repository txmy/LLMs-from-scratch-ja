# 第5章: ラベルなしデータで事前学習する

&nbsp;
## メインチャプターコード

- [01_main-chapter-code](01_main-chapter-code) メインチャプターコードを含む

&nbsp;
## ボーナス教材

- [02_alternative_weight_loading](02_alternative_weight_loading) OpenAIからモデルの重みが利用できなくなった場合に備えて、代替の場所からGPTモデルの重みを読み込むコードを含む
- [03_bonus_pretraining_on_gutenberg](03_bonus_pretraining_on_gutenberg) プロジェクト・グーテンベルクの全書籍コーパスでLLMをより長く事前学習するコードを含む
- [04_learning_rate_schedulers](04_learning_rate_schedulers) 学習率スケジューラと勾配クリッピングを含むより洗練されたトレーニング関数を実装するコードを含む
- [05_bonus_hparam_tuning](05_bonus_hparam_tuning) オプションのハイパーパラメータチューニングスクリプトを含む
- [06_user_interface](06_user_interface) 事前学習済みLLMとやり取りするためのインタラクティブなユーザーインターフェースを実装する
- [07_gpt_to_llama](07_gpt_to_llama) GPTアーキテクチャ実装をLlama 3.2に変換し、Meta AIから事前学習済み重みを読み込むステップバイステップガイドを含む
- [08_memory_efficient_weight_loading](08_memory_efficient_weight_loading) PyTorchの`load_state_dict`メソッドを使用してモデルの重みをより効率的に読み込む方法を示すボーナスノートブックを含む
- [09_extending-tokenizers](09_extending-tokenizers) GPT-2 BPEトークナイザのゼロからの実装を含む
- [10_llm-training-speed](10_llm-training-speed) LLMトレーニング速度を改善するためのPyTorchパフォーマンスのヒントを示す
- [11_qwen3](11_qwen3) Qwen3 0.6BとQwen3 30B-A3B（Mixture-of-Experts）のゼロからの実装で、ベース、推論、コーディングモデルバリアントの事前学習済み重みを読み込むコードを含む



<br>
<br>

[![Link to the video](https://img.youtube.com/vi/Zar2TJv-sE0/0.jpg)](https://www.youtube.com/watch?v=Zar2TJv-sE0)