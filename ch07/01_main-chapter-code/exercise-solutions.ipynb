{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba450fb1-8a26-4894-ab7a-5d7bfefe90ce",
   "metadata": {},
   "source": "<table style=\"width:100%\">\n<tr>\n<td style=\"vertical-align:middle; text-align:left;\">\n<font size=\"2\">\n<a href=\"http://mng.bz/orYv\">『スクラッチから大規模言語モデルを構築する』</a>（<a href=\"https://sebastianraschka.com\">Sebastian Raschka</a>著）の補助コード<br>\n<br>コードリポジトリ: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n</font>\n</td>\n<td style=\"vertical-align:middle; text-align:left;\">\n<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n</td>\n</tr>\n</table>"
  },
  {
   "cell_type": "markdown",
   "id": "51c9672d-8d0c-470d-ac2d-1271f8ec3f14",
   "metadata": {},
   "source": "# 第7章 演習解答"
  },
  {
   "cell_type": "markdown",
   "id": "2625ddc4-9cce-42bd-947d-4e2203fdc55c",
   "metadata": {},
   "source": "## 演習 7.1：プロンプトスタイルの変更"
  },
  {
   "cell_type": "markdown",
   "id": "6be25a95-2a33-433b-a698-2365b5fc9357",
   "metadata": {},
   "source": "次のデータエントリがあるとします：\n\n```json\n{\n  \"instruction\": \"Identify the correct spelling of the following word.\",\n  \"input\": \"Ocassion\",\n  \"output\": \"The correct spelling is 'Occasion.'\"\n}\n```\n\nメイン章では、これをAlpacaスタイルのプロンプトテンプレートに従ってフォーマットしました：\n\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nIdentify the correct spelling of the following word.\n\n### Input:\nOccassion\n\n### Response:\nThe correct spelling is 'Occasion.'\n```\n\nこの演習では、代わりにPhi-3プロンプトテンプレートを使用し、データエントリを次のようにフォーマットします：\n\n```\n<user>\nIdentify the correct spelling of the following word: 'Occasion'\n\n<assistant>\nThe correct spelling is 'Occasion'.\n```\n\nこのプロンプトテンプレートは大幅に短く、入力プロンプトが短いため、LLMのファインチューニングとテキスト生成の実行時間とハードウェア要件を削減できることに注意してください。\nこの変更を行うため、`format_input`関数を次のように更新します："
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99baa1e-c24c-417f-89d0-13e6d061ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"<|user|>\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba538f-64b9-495d-847b-d9f1d324bc50",
   "metadata": {},
   "source": "2つの入力サンプル（`'input'`フィールドにコンテンツがあるものとないもの）に適用して、意図した通りに動作するか確認しましょう："
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "877a57e2-535f-4363-b32a-a093edd951b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Identify the correct spelling of the following word.\n",
      "Ocassion\n",
      "\n",
      "<|user|>\n",
      "What is an antonym of 'complicated'?\n"
     ]
    }
   ],
   "source": [
    "sample_data = [\n",
    "    {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}, \n",
    "    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n",
    "]\n",
    "\n",
    "print(format_input(sample_data[0]))\n",
    "print()\n",
    "print(format_input(sample_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a6704-6c61-4a09-b8f5-ffc5a77d6aa3",
   "metadata": {},
   "source": "次に、レスポンスに<|assistant|>プロンプトテンプレートを使用するように`InstructionDataset`クラスも更新します："
  },
  {
   "cell_type": "markdown",
   "id": "81f0d9c8-8f41-4455-b9ae-6b17de610cc3",
   "metadata": {},
   "source": "```python\nimport tiktoken\nfrom torch.utils.data import Dataset\n\nclass InstructionDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n\n        # テキストを事前にトークン化\n        self.encoded_texts = []\n        for entry in data:\n\n            ###################################################################\n            # NEW: `format_input_phi`を使用し、レスポンステキストテンプレートを調整\n            instruction_plus_input = format_input(entry)\n            response_text = f\"\\n<|assistant|>:\\n{entry['output']}\"\n            ###################################################################\n            full_text = instruction_plus_input + response_text\n            self.encoded_texts.append(\n                tokenizer.encode(full_text)\n            )\n\n    def __getitem__(self, index):\n        return self.encoded_texts[index]\n\n    def __len__(self):\n        return len(self.data)\n\n\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n```"
  },
  {
   "cell_type": "markdown",
   "id": "e0650926-c39f-4442-8116-cb7494416f28",
   "metadata": {},
   "source": "最後に、テストセットのレスポンスを収集する際の生成されたレスポンスの抽出方法も更新する必要があります："
  },
  {
   "cell_type": "markdown",
   "id": "a9253041-812f-4a5f-9ab1-d7e4cb1407fb",
   "metadata": {},
   "source": "```python\nfor i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n\n    input_text = format_input(entry)\n    tokenizer=tokenizer\n\n    token_ids = generate(\n        model=model,\n        idx=text_to_token_ids(input_text, tokenizer).to(device),\n        max_new_tokens=256,\n        context_size=BASE_CONFIG[\"context_length\"],\n        eos_id=50256\n    )\n    generated_text = token_ids_to_text(token_ids, tokenizer)\n\n    # New: ###Response -> <|assistant|>に調整\n    response_text = generated_text[len(input_text):].replace(\"<|assistant|>:\", \"\").strip()\n\n    test_data[i][\"model_response\"] = response_text\n```"
  },
  {
   "cell_type": "markdown",
   "id": "29cd557c-3838-45e4-a26a-baed4b11175a",
   "metadata": {},
   "source": "ご便宜のため、演習解答は[exercise_experiments.py](exercise_experiments.py)スクリプトに実装されており、次のように実行できます："
  },
  {
   "cell_type": "markdown",
   "id": "dd8158e9-cc70-4e0f-88b0-73c3e1d8c030",
   "metadata": {},
   "source": "```bash\npython exercise_experiments.py --exercise_solution phi3_prompt\n```\n\n出力：\n\n```\nmatplotlib version: 3.7.1\ntiktoken version: 0.7.0\ntorch version: 2.3.0+cu121\ntqdm version: 4.66.4\ntensorflow version: 2.15.0\n--------------------------------------------------\nTraining set length: 935\nValidation set length: 55\nTest set length: 110\n--------------------------------------------------\nDevice: cuda\n--------------------------------------------------\n...\nLoaded model: gpt2-medium (355M)\n--------------------------------------------------\nInitial losses\n   Training loss: 3.71630220413208\n   Validation loss: 3.6440994262695314\nEp 1 (Step 000000): Train loss 2.633, Val loss 2.622\n...\nEp 2 (Step 000230): Train loss 0.424, Val loss 0.928\n<|user|> Convert the active sentence to passive: 'The chef cooks the meal every day.' <|assistant|>: The meal is prepared every day by the chef....\nTraining completed in 1.50 minutes.\nPlot saved as loss-plot-phi3-prompt.pdf\n--------------------------------------------------\nGenerating responses\n100% 110/110 [00:11<00:00,  9.27it/s]\nResponses saved as instruction-data-with-response-phi3-prompt.json\nModel saved as gpt2-medium355M-sft-phi3-prompt.pth\n```\n\n比較のため、元の第7章のファインチューニングコードは`python exercise_experiments.py --exercise_solution baseline`で実行できます。\n\nNvidia L4 GPUでは、上記のPhi-3プロンプトテンプレートを使用したコードの実行に1.5分かかることに注意してください。比較として、Alpacaスタイルテンプレートは1.80分かかります。そのため、Phi-3テンプレートはより短いモデル入力となるため、約17%高速です。\n\nレスポンスが正しくフォーマットされているか確認するために、いくつかのレスポンスを見てみましょう：\n\n```json\n    {\n        \"instruction\": \"Rewrite the sentence using a simile.\",\n        \"input\": \"The car is very fast.\",\n        \"output\": \"The car is as fast as lightning.\",\n        \"model_response\": \"The car is as fast as a cheetah.\"\n    },\n    {\n        \"instruction\": \"What type of cloud is typically associated with thunderstorms?\",\n        \"input\": \"\",\n        \"output\": \"The type of cloud typically associated with thunderstorms is cumulonimbus.\",\n        \"model_response\": \"The type of cloud associated with thunderstorms is a cumulus cloud.\"\n    },\n    {\n        \"instruction\": \"Name the author of 'Pride and Prejudice'.\",\n        \"input\": \"\",\n        \"output\": \"Jane Austen.\",\n        \"model_response\": \"The author of 'Pride and Prejudice' is Jane Austen.\"\n    },\n```\n\nご便宜のため、`python exercise_experiments.py`スクリプトにも実装されているOllama Llama 3手法を使用してパフォーマンスを評価できます。次のように実行できます：\n\n```bash\npython ollama_evaluate.py --file_path instruction-data-with-response-phi3-prompt.json\n```\n\n出力：\n\n```\nOllama running: True\nScoring entries: 100%|████████████████████████| 110/110 [01:08<00:00,  1.60it/s]\nNumber of scores: 110 of 110\nAverage score: 48.87\n```\n\nスコアは50に近く、これは以前にAlpacaスタイルプロンプトで達成したスコアと同程度です。\n\nPhiプロンプトスタイルが優れている固有の利点や根拠はありませんが、以下の*ヒント*セクションで述べる注意点を除けば、より簡潔で効率的である可能性があります。"
  },
  {
   "cell_type": "markdown",
   "id": "156bc574-3f3e-4479-8f58-c8c8c472416e",
   "metadata": {},
   "source": "#### ヒント：特殊トークンの検討"
  },
  {
   "cell_type": "markdown",
   "id": "65cacf90-21c2-48f2-8f21-5c0c86749ff2",
   "metadata": {},
   "source": "- Phi-3プロンプトテンプレートには`<|user|>`や`<|assistant|>`などの特殊トークンが含まれており、これらはGPT-2トークナイザーにとって準最適である可能性があることに注意してください\n- GPT-2トークナイザーは`<|endoftext|>`を特殊トークン（トークンID 50256にエンコード）として認識しますが、前述のような他の特殊トークンの処理は非効率です\n- 例えば、`<|user|>`は5つの個別のトークンID（27, 91, 7220, 91, 29）にエンコードされ、これは非常に非効率です\n- `allowed_special`引数を介して`tiktoken`に新しい特殊トークンとして`<|user|>`を追加することもできますが、GPT-2の語彙は追加の修正なしにはそれを処理できないことに注意してください\n- トークナイザーとLLMを特殊トークンを処理するように拡張する方法に興味がある場合は、[extend-tiktoken.ipynb](../../ch05/09_extending-tokenizers/extend-tiktoken.ipynb)のボーナス資料をご覧ください（これはここでは必須ではありませんが、好奇心旺盛な読者にとって興味深い/ボーナス的な検討事項です）\n- さらに、プロンプトテンプレートのこれらの特殊トークンを語彙を介してサポートするモデルは、より効率的で全体的に優れたパフォーマンスを発揮する可能性があると仮説を立てることができます"
  },
  {
   "cell_type": "markdown",
   "id": "5fea8be3-30a1-4623-a6d7-b095c6c1092e",
   "metadata": {},
   "source": "&nbsp;\n## 演習 7.2：命令と入力のマスク\n\n次の図に示すように命令をマスクアウトするには、`InstructionDataset`クラスと`custom_collate_fn`に軽微な変更を加える必要があります。\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/mask-instructions.webp\" width=600px>"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4405196a-db81-470b-be39-167a059587b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This `format_input` function is copied from the original chapter 7 code\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83658c09-af8a-425a-b940-eb1f06e43c0b",
   "metadata": {},
   "source": "`InstructionDataset`クラスを変更して命令の長さを収集し、これをcollate関数で使用してターゲットで命令コンテンツの位置を特定し、collate関数をコーディングする際に使用できます："
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e6188a-f182-4f26-b9e5-ccae3ecadae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        ##########################################################################################\n",
    "        # New: Separate list for instruction lengths\n",
    "        self.instruction_lengths = []\n",
    "        ##########################################################################################\n",
    "        \n",
    "        self.encoded_texts = []\n",
    "        \n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            \n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "            ##########################################################################################\n",
    "            # New: collect instruction lengths\n",
    "            instruction_length = len(tokenizer.encode(instruction_plus_input))\n",
    "            self.instruction_lengths.append(instruction_length)\n",
    "            ##########################################################################################\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # New: return both instruction lengths and texts separately\n",
    "        return self.instruction_lengths[index], self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0163b7d1-acb8-456c-8efe-86307b58f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a186394-4960-424d-bb6a-f58459dd5994",
   "metadata": {},
   "source": "次に、`InstructionDataset`データセットの変更により、各`batch`が単なる`item`ではなく`(instruction_length, item)`を含むタプルとなる`custom_collate_fn`を更新します。さらに、ターゲットIDリストで対応する命令トークンをマスクします。"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f815e6fc-8e54-4105-aecd-d4c6e890ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for instruction_length, item in batch)   # New: batch is now a tuple\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for instruction_length, item in batch:  # New: batch is now a tuple\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        # Replace all but the first padding tokens in targets by ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "\n",
    "        ##########################################################################################\n",
    "        # New: Mask all input and instruction tokens in the targets\n",
    "        targets[:instruction_length-1] = -100\n",
    "        ##########################################################################################\n",
    "        \n",
    "        # Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a4815-850e-42c4-b70d-67e8ce5ebd57",
   "metadata": {},
   "source": "以下のサンプルデータで試してみましょう："
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da8a5b1-a8e2-4389-b21c-25b67be6dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"},\n",
    "    {'instruction': 'Sort the following list in alphabetical order.', 'input': 'Zebra, Elephant, Crocodile', 'output': 'Crocodile, Elephant, Zebra'},\n",
    "    {'instruction': 'Arrange the given numbers in descending order.', 'input': '5, 12, 8, 3, 15', 'output': '15, 12, 8, 5, 3.'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "435b0816-0fc8-4650-a84a-eceffa4d85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = InstructionDataset(sample_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=len(sample_data),\n",
    "    collate_fn=custom_collate_fn,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106bbbd7-7286-4eb6-b343-43419332a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([3, 64]) torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bb3288b-84a9-4962-ae59-a7a29fd34bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 42758,   262,  1708,  1351,   287, 24830,\n",
      "          605,  1502,    13,   198,   198, 21017, 23412,    25,   198,    57,\n",
      "        37052,    11, 42651,    11,  9325, 19815,   576,   198,   198, 21017,\n",
      "        18261,    25,   198,    34, 12204,   375,   576,    11, 42651,    11,\n",
      "         1168, 37052, 50256, 50256])\n",
      "\n",
      "\n",
      "Targets:\n",
      " tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,   198,   198, 21017, 18261,\n",
      "           25,   198,    34, 12204,   375,   576,    11, 42651,    11,  1168,\n",
      "        37052, 50256,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(\"Inputs:\\n\", inputs[1])\n",
    "print(\"\\n\\nTargets:\\n\", targets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40347b-2ca7-44e1-862d-0fd0c92f0628",
   "metadata": {},
   "source": "`targets`テンソルに基づいて、命令とパディングトークンの両方が-100プレースホルダートークンを使用してマスクされていることがわかります。\n入力が正しく見えるか確認するために、入力をデコードしてみましょう："
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a9e6fa-3d75-4e39-b139-c3e05048f42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Zebra, Elephant, Crocodile\n",
      "\n",
      "### Response:\n",
      "Crocodile, Elephant, Zebra<|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(list(inputs[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ebd36-f63f-4b58-a76e-7767e4d2ccbd",
   "metadata": {},
   "source": "次に、マスクされていないターゲットトークンIDをデコードしてみましょう："
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d54a152-b778-455a-8941-e375e2a17e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### Response:\n",
      "Crocodile, Elephant, Zebra<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "non_masked_targets = targets[1][targets[1] != -100]\n",
    "\n",
    "print(tokenizer.decode(list(non_masked_targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3912bbf5-e9e2-474b-9552-d522e7510aa6",
   "metadata": {},
   "source": "上記に示すように、マスクされていないターゲットトークンは意図した通り`\"Instruction\"`と`\"Input\"`フィールドを除外しています。これで、このマスキング戦略を使用してファインチューニングされたときのLLMのパフォーマンスを確認するために、修正されたコードを実行できます。\n\nご便宜のため、`exercise_experiments.py`コードを使用して次のような比較を実行できます："
  },
  {
   "cell_type": "markdown",
   "id": "56a76097-9114-479d-8803-443b0ff48581",
   "metadata": {},
   "source": "```bash\npython exercise_experiments.py --exercise_solution mask_instructions\n```\n\n出力：\n\n```\nmatplotlib version: 3.7.1\ntiktoken version: 0.7.0\ntorch version: 2.3.0+cu121\ntqdm version: 4.66.4\ntensorflow version: 2.15.0\n--------------------------------------------------\nTraining set length: 935\nValidation set length: 55\nTest set length: 110\n--------------------------------------------------\nDevice: cuda\n--------------------------------------------------\n...\nLoaded model: gpt2-medium (355M)\n--------------------------------------------------\nInitial losses\n   Training loss: 2.280539035797119\n   Validation loss: 2.262560224533081\nEp 1 (Step 000000): Train loss 1.636, Val loss 1.620\n...\nEp 2 (Step 000230): Train loss 0.143, Val loss 0.727\n...\nTraining completed in 1.77 minutes.\nPlot saved as loss-plot-mask-instructions.pdf\n--------------------------------------------------\nGenerating responses\n100% 110/110 [02:10<00:00,  1.19s/it]\nResponses saved as instruction-data-with-response-mask-instructions.json\nModel saved as gpt2-medium355M-sft-mask-instructions.pth\n```\n\n次に、結果のLLMのパフォーマンスを評価しましょう：\n\n```bash\npython ollama_evaluate.py --file_path instruction-data-with-response-mask-instructions.json\n```\n\n```\nOllama running: True\nScoring entries: 100%|██████████████████████████████████████████████████████████████████████████████████████| 110/110 [01:23<00:00,  1.31it/s]\nNumber of scores: 110 of 110\nAverage score: 47.73\n```\n\nスコアに基づいて、命令マスキングの性能はわずかに劣っており、これは「Instruction Tuning With Loss Over Instructions」論文（https://arxiv.org/abs/2405.14394）での観察と一致しています"
  },
  {
   "cell_type": "markdown",
   "id": "94a0f758-29da-44ee-b7af-32473b3c086e",
   "metadata": {},
   "source": "&nbsp;\n## 演習 7.3：元のAlpacaデータセットでのファインチューニング"
  },
  {
   "cell_type": "markdown",
   "id": "68df7616-679f-4e53-954d-6e7cf2e2ef55",
   "metadata": {},
   "source": "元のStanford Alpacaデータセット（[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)）でモデルをファインチューニングするには、ファイルURLを\n\n```python\nurl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n```\n\nから\n\n```python\nurl = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\"\n```\n\nに変更するだけです。\n\nデータセットには52kエントリ（第7章で使用したものの50倍）が含まれており、エントリは第7章で使用したものより長いことに注意してください。\nそのため、GPUでトレーニングを実行することを強く推奨します。\n\nメモリ不足エラーが発生した場合は、バッチサイズを8から4、2、または1に減らすことを検討してください。バッチサイズを下げることに加えて、`allowed_max_length`を1024から512または256に下げることも検討できます。"
  },
  {
   "cell_type": "markdown",
   "id": "d94c9621-2c3f-4551-b5b8-87cd96e38c9c",
   "metadata": {},
   "source": "ご便宜のため、`exercise_experiments.py`コードを使用して、バッチサイズ4と`allowed_max_length`を512に設定して52k Alpacaデータセットでモデルをファインチューニングできます："
  },
  {
   "cell_type": "markdown",
   "id": "40a76486-73e6-4415-94dc-bfe2aa36ea52",
   "metadata": {},
   "source": "```bash\npython exercise_experiments.py --exercise_solution alpaca_52k\n```\n\n```\nmatplotlib version: 3.7.1\ntiktoken version: 0.7.0\ntorch version: 2.3.0+cu121\ntqdm version: 4.66.4\ntensorflow version: 2.15.0\n--------------------------------------------------\nTraining set length: 44201\nValidation set length: 2601\nTest set length: 5200\n--------------------------------------------------\nDevice: cuda\n--------------------------------------------------\n...\nLoaded model: gpt2-medium (355M)\n--------------------------------------------------\nInitial losses\n   Training loss: 3.3681655883789063\n   Validation loss: 3.4122894287109373\nEp 1 (Step 000000): Train loss 2.477, Val loss 2.750\n...\nEp 2 (Step 022095): Train loss 0.761, Val loss 1.557\n...\nTraining completed in 196.38 minutes.\nPlot saved as loss-plot-alpaca52k.pdf\n--------------------------------------------------\nGenerating responses\n100% 5200/5200 [2:56:33<00:00,  2.04s/it]\nResponses saved as instruction-data-with-response-alpaca52k.json\nModel saved as gpt2-medium355M-sft-alpaca52k.pth\n```"
  },
  {
   "cell_type": "markdown",
   "id": "cc8a1bd1-15f4-4a35-87b5-369038db0aa7",
   "metadata": {},
   "source": "以下は、生成されたモデルレスポンスを含むAlpacaデータセットからのいくつかの例です："
  },
  {
   "cell_type": "markdown",
   "id": "be9ab66f-5819-4b01-9a03-c45aa3b7c5b8",
   "metadata": {},
   "source": [
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Edit the following sentence to increase readability: \\\"He made a huge effort and was so successful.\\\"\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"He exerted a tremendous effort, and thus enjoyed great success.\",\n",
    "        \"model_response\": \"He put in an immense effort and was rewarded with success.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Rewrite the following sentence to make it more concise: \\\"I was displeased with the result of the experiment that I conducted.\\\"\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"I was unhappy with my experiment's outcome.\",\n",
    "        \"model_response\": \"I was displeased with the results of the experiment.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How can we build a more efficient GPT model?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"We can build a more efficient GPT model by optimizing the architecture of the model, using smaller model sizes and training with fewer parameters. We can also leverage techniques such as knowledge distillation, transfer learning, dynamic sparsity and hybrid computing to further improve the efficiency of the model.\",\n",
    "        \"model_response\": \"Building a more efficient GPT model requires careful planning and optimization. First, it is important to identify the target language and the context in which the model is used. Then, it is important to select the appropriate model architecture, such as backpropagation, hyperparameters, and hyperparameters. Finally, it is important to select the appropriate model weights and optimizers, such as backpropagation, hyperparameters, and hyperparameters.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e2076-f2e6-44a2-86fd-f191f9674267",
   "metadata": {},
   "source": "最後に、[ollama_evaluate.py](ollama_evaluate.py)ユーティリティ関数を使用してファインチューニングされたLLMを評価できます：\n\n```bash\npython ollama_evaluate.py --file_path instruction-data-with-response-alpaca52k.json\n```\n\n```\nScoring entries: 100%|████████████████████| 5200/5200 [1:07:52<00:00, 1.28it/s]\nNumber of scores: 5188 of 5200\nAverage score: 48.16\n```"
  },
  {
   "cell_type": "markdown",
   "id": "d14b3c60-00a1-43a9-9fcd-592aaadf1ef4",
   "metadata": {},
   "source": "スコアは、この章で使用したデータセットで得られたスコアよりもわずかに低くなっています。ただし、Alpacaテストセットには、メイン章で使用したデータセットよりも多様で、一部はより困難な命令が含まれていることに注意してください。"
  },
  {
   "cell_type": "markdown",
   "id": "ca61fa6c-4e1d-4618-9e5e-d091f8303e30",
   "metadata": {},
   "source": "## 演習 7.4：LoRAによるパラメータ効率的ファインチューニング"
  },
  {
   "cell_type": "markdown",
   "id": "01742cec-1f41-4415-8788-009d31b1ad38",
   "metadata": {},
   "source": "LoRAを使用してモデルを命令ファインチューニングするには、付録Eから関連するクラスと関数を使用します：\n\n```python\nfrom appendix_E import LoRALayer, LinearWithLoRA, replace_linear_with_lora\n```"
  },
  {
   "cell_type": "markdown",
   "id": "871dca8f-3411-4735-b7b0-9d0e6e0599ac",
   "metadata": {},
   "source": "次に、セクション7.5のモデル読み込みコードの下に以下のコード行を追加します：\n\n\n```python\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters before: {total_params:,}\")\n\nfor param in model.parameters():\n    param.requires_grad = False\n\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters after: {total_params:,}\")\nreplace_linear_with_lora(model, rank=16, alpha=16)\n\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable LoRA parameters: {total_params:,}\")\nmodel.to(device)\n```"
  },
  {
   "cell_type": "markdown",
   "id": "1b26b925-dc95-4b91-b050-9676dd9608a4",
   "metadata": {},
   "source": "ご便宜のため、`exercise_experiments.py`コードを使用して、rank 16とalpha 16のLoRAでモデルをファインチューニングできます："
  },
  {
   "cell_type": "markdown",
   "id": "01f02c7e-3b15-44b8-bf41-7892cd755766",
   "metadata": {},
   "source": "```bash\npython exercise_experiments.py --exercise_solution lora\n```\n\n出力：\n\n```\nmatplotlib version: 3.7.1\ntiktoken version: 0.7.0\ntorch version: 2.3.0+cu121\ntqdm version: 4.66.4\ntensorflow version: 2.15.0\n--------------------------------------------------\nTraining set length: 935\nValidation set length: 55\nTest set length: 110\n--------------------------------------------------\nDevice: cuda\n--------------------------------------------------\nFile already exists and is up-to-date: gpt2/355M/checkpoint\nFile already exists and is up-to-date: gpt2/355M/encoder.json\nFile already exists and is up-to-date: gpt2/355M/hparams.json\nFile already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\nFile already exists and is up-to-date: gpt2/355M/model.ckpt.index\nFile already exists and is up-to-date: gpt2/355M/model.ckpt.meta\nFile already exists and is up-to-date: gpt2/355M/vocab.bpe\nLoaded model: gpt2-medium (355M)\n--------------------------------------------------\nTotal trainable parameters before: 406,286,336\nTotal trainable parameters after: 0\nTotal trainable LoRA parameters: 7,898,384\nInitial losses\n   Training loss: 3.7684114456176756\n   Validation loss: 3.7619335651397705\nEp 1 (Step 000000): Train loss 2.509, Val loss 2.519\n...\nEp 2 (Step 000230): Train loss 0.308, Val loss 0.652\n...\n--------------------------------------------------\nGenerating responses\n100% 110/110 [01:52<00:00,  1.03s/it]\nResponses saved as instruction-data-with-response-lora.json\nModel saved as gpt2-medium355M-sft-lora.pth\n```\n\n比較のため、元の第7章のファインチューニングコードは`python exercise_experiments.py --exercise_solution baseline`で実行できます。\n\nNvidia L4 GPUでは、上記のLoRAを使用したコードの実行に1.30分かかることに注意してください。比較として、ベースラインは1.80分かかります。そのため、LoRAは約28%高速です。\n\n\nご便宜のため、`python exercise_experiments.py`スクリプトにも実装されているOllama Llama 3手法を使用してパフォーマンスを評価できます。次のように実行できます：\n\n```bash\npython ollama_evaluate.py --file_path instruction-data-with-response-lora.json\n```\n\n出力：\n\n```\nOllama running: True\nScoring entries: 100%|████████████████████████| 110/110 [01:13<00:00,  1.50it/s]\nNumber of scores: 110 of 110\nAverage score: 50.23\n```\n\nスコアは50付近で、元のモデルと同程度です。"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}