{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "136a4efe-fb99-4311-8679-e0a5b6282755",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1910a06-e8a3-40ac-8201-ff70615b1ba4",
   "metadata": {
    "tags": []
   },
   "source": "# Llama 3.1 70BとOllamaを使用した選好データセットの生成"
  },
  {
   "cell_type": "markdown",
   "id": "a128651b-f326-4232-a994-42f38b7ed520",
   "metadata": {},
   "source": "- 選好ファインチューニングは、指示ファインチューニングされたLLMを人間の選好に合わせるプロセスです\n- LLMの選好ファインチューニングのためのデータセットを作成する複数の方法があります\n  1. 指示ファインチューニングされたLLMを使用して複数の応答を生成し、人間にそれらを選好や所定の選好基準に基づいてランク付けしてもらう\n  2. 指示ファインチューニングされたLLMを使用して複数の応答を生成し、LLMに所定の選好基準に基づいてそれらをランク付けしてもらう\n  3. LLMを使用して、特定の選好基準に基づいて好ましい応答と好ましくない応答を生成する\n- このノートブックでは、アプローチ3を検討します\n- このノートブックは、ollama経由で700億パラメータのLlama 3.1-Instructモデルを使用して、指示データセットの選好ラベルを生成します\n- 指示データセットの期待される形式は以下の通りです：\n\n\n### 入力\n\n```json\n[\n    {\n        \"instruction\": \"カリフォルニア州の州都は何ですか？\",\n        \"input\": \"\",\n        \"output\": \"カリフォルニア州の州都はサクラメントです。\",\n    },\n    {\n        \"instruction\": \"'fast'の類義語を教えてください。\",\n        \"input\": \"\",\n        \"output\": \"'fast'の類義語は'quick'です。\",\n    },\n    {\n        \"instruction\": \"ギリシャの首都は何ですか？\",\n        \"input\": \"\",\n        \"output\": \"ギリシャの首都はアテネです。\",\n\n    },\n...\n]\n```\n\n出力データセットは以下のようになります。より丁寧な応答が好ましい（`'chosen'`）とされ、より無礼な応答が好ましくない（`'rejected'`）とされます：\n\n```json\n[\n    {\n        \"instruction\": \"カリフォルニア州の州都は何ですか？\",\n        \"input\": \"\",\n        \"output\": \"カリフォルニア州の州都はサクラメントです。\",\n        \"rejected\": \"見てください、カリフォルニア州の州都は明らかにサクラメントです。\",\n        \"chosen\": \"カリフォルニア州の州都はサクラメントです。\"\n    },\n    {\n        \"instruction\": \"'fast'の類義語を教えてください。\",\n        \"input\": \"\",\n        \"output\": \"'fast'の類義語は'quick'です。\",\n        \"chosen\": \"'fast'の適切な代替語は'quick'でしょう。\",\n        \"rejected\": \"'fast'の類義語は'quick'です。\"\n    },\n    {\n        \"instruction\": \"ギリシャの首都は何ですか？\",\n        \"input\": \"\",\n        \"output\": \"ギリシャの首都はアテネです。\",\n        \"chosen\": \"喜んでお答えします！ギリシャの首都は確かにアテネです。\",\n        \"rejected\": \"ギリシャの首都はアテネです。\"\n    },\n...\n]\n```\n\n### 出力\n\n\n\n\n- このコードはGPUを必要とせず、十分なRAMがあればラップトップで実行できます"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63610acc-db94-437f-8d38-e99dca0299cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm version: 4.66.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"tqdm\",    # Progress bar\n",
    "        ]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcb34-ac75-4f4f-9505-3ce0666c42d5",
   "metadata": {},
   "source": "## OllamaのインストールとLlama 3.1のダウンロード"
  },
  {
   "cell_type": "markdown",
   "id": "5a092280-5462-4709-a3fe-8669a4a8a0a6",
   "metadata": {},
   "source": "- OllamaはLLMを効率的に実行するアプリケーションです\n- これは[llama.cpp](https://github.com/ggerganov/llama.cpp)のラッパーで、LLMを純粋なC/C++で実装して効率を最大化します\n- これはLLMを使用してテキストを生成する（推論）ためのツールであり、LLMの訓練やファインチューニングのためのものではないことに注意してください\n- 以下のコードを実行する前に、[https://ollama.com](https://ollama.com)にアクセスして指示に従ってollamaをインストールしてください（例えば、「Download」ボタンをクリックして、お使いのオペレーティングシステム用のollamaアプリケーションをダウンロード）"
  },
  {
   "cell_type": "markdown",
   "id": "9558a522-650d-401a-84fc-9fd7b1f39da7",
   "metadata": {},
   "source": "- macOSおよびWindowsユーザーの場合、ダウンロードしたollamaアプリケーションをクリックしてください。コマンドラインの使用をインストールするかを尋ねられた場合は「yes」と答えてください\n- Linuxユーザーは、ollamaウェブサイトで提供されているインストールコマンドを使用できます\n\n- 一般的に、コマンドラインからollamaを使用する前に、ollamaアプリケーションを開始するか、別のターミナルで`ollama serve`を実行する必要があります\n\n<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/ollama-eval/ollama-serve.webp?1\">\n\n\n- ollamaアプリケーションまたは`ollama serve`が実行されている状態で、別のターミナルで、コマンドラインで以下のコマンドを実行して700億パラメータのLlama 3.1モデルを試してください\n\n```bash\n# 70Bモデル\nollama run llama3.1:70b\n```\n\n\n出力は以下のようになります：\n\n```\n$ ollama run llama3.1:70b\npulling manifest\npulling aa81b541aae6... 100% ▕████████████████▏ 39 GB\npulling 8cf247399e57... 100% ▕████████████████▏ 1.7 KB\npulling f1cd752815fc... 100% ▕████████████████▏ 12 KB\npulling 56bb8bd477a5... 100% ▕████████████████▏ 96 B\npulling 3c1c2d3df5b3... 100% ▕████████████████▏ 486 B\nverifying sha256 digest\nwriting manifest\nremoving any unused layers\nsuccess\n```\n\n- `llama3.1:70b`は指示ファインチューニングされた700億パラメータのLlama 3.1モデルを指すことに注意してください\n\n- あるいは、`llama3.1:70b`を`llama3.1`に置き換えることで、より小さく、よりリソース効率的な80億パラメータのLlama 3.1モデルを使用することもできます\n\n- ダウンロードが完了すると、モデルとチャットできるコマンドラインプロンプトが表示されます\n\n- 「What do llamas eat?」のようなプロンプトを試してみてください。以下のような出力が返されるはずです：\n\n```\n>>> What do llamas eat?\nLlamas are ruminant animals, which means they have a four-chambered \nstomach and eat plants that are high in fiber. In the wild, llamas \ntypically feed on:\n1. Grasses: They love to graze on various types of grasses, including tall \ngrasses, wheat, oats, and barley.\n```"
  },
  {
   "cell_type": "markdown",
   "id": "0b5addcb-fc7d-455d-bee9-6cc7a0d684c7",
   "metadata": {},
   "source": "- `/bye`と入力してこのセッションを終了できます"
  },
  {
   "cell_type": "markdown",
   "id": "dda155ee-cf36-44d3-b634-20ba8e1ca38a",
   "metadata": {},
   "source": "## OllamaのREST APIの使用"
  },
  {
   "cell_type": "markdown",
   "id": "89343a84-0ddc-42fc-bf50-298a342b93c0",
   "metadata": {},
   "source": "- さて、モデルと対話する別の方法として、以下の関数を使ってPythonでREST APIを使用する方法があります\n- このノートブックの次のセルを実行する前に、上記で説明したように以下のいずれかでollamaが実行されていることを確認してください：\n  - ターミナルで`ollama serve`\n  - ollamaアプリケーション\n- 次に、以下のコードセルを実行してモデルにクエリを送信してください"
  },
  {
   "cell_type": "markdown",
   "id": "16642a48-1cab-40d2-af08-ab8c2fbf5876",
   "metadata": {},
   "source": "- まず、APIが意図した通りに動作することを確認するために、簡単な例でAPIを試してみましょう："
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b0ba76-1fb1-4306-a7c2-8f3bb637ccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet consists of:\n",
      "\n",
      "1. **Grasses**: Various types of grasses, including timothy grass, orchard grass, and brome grass.\n",
      "2. **Hay**: High-quality hay, such as alfalfa or clover hay, is a staple in a llama's diet.\n",
      "3. **Leaves**: Leaves from trees and shrubs, like willow, cottonwood, and mesquite, are also eaten.\n",
      "4. **Fruits and vegetables**: Llamas enjoy fruits like apples, carrots, and sweet potatoes, as well as leafy greens like kale and spinach.\n",
      "5. **Grains**: In moderation, llamas can eat grains like oats, barley, and corn.\n",
      "\n",
      "It's essential to note that llamas have a unique digestive system, with a three-part stomach and a large cecum (a specialized part of the large intestine). This allows them to break down and extract nutrients from plant material more efficiently than many other animals.\n",
      "\n",
      "A typical llama diet might consist of:\n",
      "\n",
      "* 1-2% of their body weight in hay per day\n",
      "* 0.5-1% of their body weight in grains per day (if fed)\n",
      "* Free-choice access to fresh water\n",
      "* Limited amounts of fruits and vegetables as treats\n",
      "\n",
      "It's also important to ensure that llamas have access to a mineral supplement, such as a salt lick or loose minerals, to help maintain optimal health.\n",
      "\n",
      "Remember, every llama is different, and their dietary needs may vary depending on factors like age, size, and activity level. Consult with a veterinarian or experienced llama breeder for specific guidance on feeding your llama.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "\n",
    "def query_model(prompt, model=\"llama3.1:70b\", url=\"http://localhost:11434/api/chat\"):\n",
    "    # Create the data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    # Create a request object, setting the method to POST and adding necessary headers\n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # Send the request and capture the response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        # Read and decode the response\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "result = query_model(\"What do Llamas eat?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162a4739-6f03-4092-a5c2-f57a0b6a4c4d",
   "metadata": {},
   "source": "## JSONエントリの読み込み"
  },
  {
   "cell_type": "markdown",
   "id": "ca011a8b-20c5-4101-979e-9b5fccf62f8a",
   "metadata": {},
   "source": "- さて、データ生成部分に取り掛かりましょう\n- ここでは、実践的な例として、第7章でモデルを指示ファインチューニングする際に元々使用した`instruction-data.json`ファイルを使用します："
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2d393a-aa92-4190-9d44-44326a6f699b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "json_file = Path(\"..\", \"01_main-chapter-code\", \"instruction-data.json\")\n",
    "\n",
    "with open(json_file, \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Number of entries:\", len(json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9751b-59b7-43fe-acc7-14e8daf2fa66",
   "metadata": {},
   "source": "- このファイルの構造は以下の通りです。テストデータセットの指定された応答（`'output'`）があり、これを`'input'`と`'instruction'`に基づいて指示ファインチューニングによってモデルが生成するように訓練しました"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7222fdc0-5684-4f2b-b741-3e341851359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Evaluate the following phrase by transforming it into the spelling given.',\n",
       " 'input': 'freind --> friend',\n",
       " 'output': 'The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0331b-6024-4bba-89a9-a088b14a1046",
   "metadata": {},
   "source": "- 以下は、指示と入力をフォーマットする小さなユーティリティ関数です："
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43263cd3-e5fb-4ab5-871e-3ad6e7d21a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that \"\n",
    "        f\"appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    instruction_text + input_text\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a55283-7d51-4136-ba60-f799d49f4098",
   "metadata": {},
   "source": "- さて、ollama APIを使用してモデル選好調整のための`'chosen'`と`'rejected'`応答を生成してみましょう\n- ここでは、説明目的で、より丁寧またはより無礼な回答を作成します"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735cc089-d127-480a-b39d-0782581f0c41",
   "metadata": {},
   "outputs": [],
   "source": "import random\n\n\nfor entry in json_data[:5]:\n    \n    politeness = random.choice([\"polite\", \"impolite\"])    \n    prompt = (\n        f\"入力 `{format_input(entry)}` \"\n        f\"と正しい出力 `{entry['output']}` が与えられたとき、\"\n        f\"出力をより{politeness}になるように少し書き直してください。\"\n        \"変更は最小限に留めてください。\"\n        \"生成された応答のみを返し、他は何も返さないでください。\"\n    )\n    print(\"\\nデータセット応答:\")\n    print(\">>\", entry['output'])\n    print(f\"\\n{politeness} 応答:\")\n    print(\">>\", query_model(prompt))    "
  },
  {
   "cell_type": "markdown",
   "id": "142dfaa7-429f-4eb0-b74d-ff327f79547a",
   "metadata": {},
   "source": "- 上記で生成された応答が妥当に見える場合は、次のステップに進んで、データセット全体にプロンプトを適用できます\n- ここでは、好ましい応答に`'chosen'`キー、好ましくない応答に`'rejected'`キーを追加します"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349dbbc-963f-4af3-9790-12dbfdca63c3",
   "metadata": {},
   "outputs": [],
   "source": "import random\nfrom tqdm import tqdm\n\ndef generate_model_responses(json_data):\n\n    for i, entry in enumerate(tqdm(json_data, desc=\"エントリを書き込み中\")):\n        politeness = random.choice([\"polite\", \"impolite\"])    \n        prompt = (\n            f\"入力 `{format_input(entry)}` \"\n            f\"と正しい出力 `{entry['output']}` が与えられたとき、\"\n            f\"出力をより{politeness}になるように少し書き直してください。\"\n            \"変更は最小限に留めてください。\"\n            \"生成された応答のみを返し、他は何も返さないでください。\"\n        )\n        response = query_model(prompt)\n        \n        if politeness == \"polite\":\n            json_data[i][\"chosen\"] = response\n            json_data[i][\"rejected\"] = entry[\"output\"]\n        else:\n            json_data[i][\"rejected\"] = response\n            json_data[i][\"chosen\"] = entry[\"output\"]    "
  },
  {
   "cell_type": "markdown",
   "id": "b071ce84-1866-427f-a272-b46700f364b2",
   "metadata": {},
   "source": "- さあ、データセット全体にこの評価を適用して、各モデルの平均スコアを計算しましょう（M3 MacBook Airラップトップで約17分かかります）\n- ollamaは（この記事執筆時点で）オペレーティングシステム間で完全に決定論的ではないため、得られる数値は以下で示されるものと若干異なる場合があります"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f700d4b-19e5-4404-afa7-b0f093024232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing entries: 100%|██████████| 1100/1100 [17:20<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_model_responses(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838d9747-0f7d-46fe-aab5-9ee6b765d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instruction-data-with-preference.json\", \"w\") as file:\n",
    "    json.dump(json_data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}